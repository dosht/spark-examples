{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:/docker/lib/ch06-lsa-1.0.2-jar-with-dependencies.jar\n",
      "Finished download of ch06-lsa-1.0.2-jar-with-dependencies.jar\n",
      "Types will be printed.\n"
     ]
    }
   ],
   "source": [
    "%AddJar file:/docker/lib/ch06-lsa-1.0.2-jar-with-dependencies.jar\n",
    "%showtypes on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the fat xml and convert it to RDDs of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at map at <console>:33"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.cloudera.datascience.common.XmlInputFormat\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.io._\n",
    "\n",
    "val path = \"file:///docker/datasets/wikidump/wiki-latest.xml\"\n",
    "@transient val conf = new Configuration()\n",
    "conf.set(XmlInputFormat.START_TAG_KEY, \"<page>\")\n",
    "conf.set(XmlInputFormat.END_TAG_KEY, \"</page>\")\n",
    "\n",
    "val kvs = sc.newAPIHadoopFile(path, classOf[XmlInputFormat], classOf[LongWritable], classOf[Text], conf).sample(false, 0.00003)\n",
    "val rawXmls = kvs.map(p => p._2.toString)\n",
    "rawXmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import edu.umd.cloud9.collection.wikipedia.language._\n",
    "import edu.umd.cloud9.collection.wikipedia._\n",
    "\n",
    "def wikiXmlToPlainText(xml: String): Option[(S$tring, String)] = {\n",
    "    val page = new EnglishWikipediaPage()\n",
    "    WikipediaPage.readPage(page, xml)\n",
    "    if (page.isEmpty) None\n",
    "    else Some(page.getTitle -> page.getContent)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val plainText = rawXmls.flatMap(wikiXmlToPlainText).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plainText.saveAsTextFile(\"WikiText\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import edu.stanford.nlp.pipeline._\n",
    "import edu.stanford.nlp.ling.CoreAnnotations._\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.collection.JavaConverters._\n",
    "\n",
    "def createNLPPipeline(): StanfordCoreNLP = {\n",
    "    val props = new java.util.Properties()\n",
    "    props.put(\"annotators\", \"tokenize, ssplit, pos, lemma\")\n",
    "    new StanfordCoreNLP(props)\n",
    "}\n",
    "\n",
    "def isOnlyLetters(str: String): Boolean = str forall Character.isLetter\n",
    "\n",
    "def plainTextToLemmas(text: String, stopWords: Set[String], pipline: StanfordCoreNLP): Seq[String] = {\n",
    "    val doc = new Annotation(text)\n",
    "    pipline.annotate(doc)\n",
    "    val lemmas = new ArrayBuffer[String]()\n",
    "    val sentences = doc.get(classOf[SentencesAnnotation]).asScala\n",
    "    for {\n",
    "        sentence <- sentences\n",
    "        token <- sentence.get(classOf[TokensAnnotation]).asScala\n",
    "        } {\n",
    "        val lemma = token.get(classOf[LemmaAnnotation])\n",
    "        if (lemma.length > 2 && !stopWords(lemma) && isOnlyLetters(lemma)) {\n",
    "            lemmas += lemma.toLowerCase\n",
    "        }\n",
    "    }\n",
    "    lemmas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val stopWords = sc.broadcast(scala.io.Source.fromFile(\"/docker/Spark/stopwords.txt\").getLines.toSet).value\n",
    "\n",
    "val lemmatized: RDD[Seq[String]] = plainText.mapPartitions { it => \n",
    "    val pipeline = createNLPPipeline()\n",
    "    it.map { case (title, contents) => plainTextToLemmas(contents, stopWords, pipeline) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scala.collection.immutable.Set[String] = Set(down, it's, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, ours\tourselves, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, with,..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Long = 70"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized.cache()\n",
    "lemmatized.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "docTermFreqs.type = MapPartitionsRDD[6] at map at <console>:63"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.HashMap\n",
    "\n",
    "val docTermFreqs = lemmatized.map(terms => terms.foldLeft(\n",
    "    new HashMap[String, Int]())((map, term) => map += term -> (map.getOrElse(term, 0) + 1)))\n",
    "docTermFreqs.cache() // Important because it will be used more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "docTermFreqs.type = MapPartitionsRDD[257] at map at <console>:75"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.HashMap\n",
    "\n",
    "val docTermFreqs = lemmatized.map {\n",
    "terms =>\n",
    "    val termFreqsInDoc = terms.foldLeft(new HashMap[String, Int]()) {\n",
    "        (map, term) => map += term -> (map.getOrElse(term, 0) + 1)\n",
    "    }\n",
    "    termFreqsInDoc\n",
    "}\n",
    "\n",
    "docTermFreqs.cache() // Important because it will be used more than once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using *aggrigate* but may throws OutOfMemoryException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scala.collection.mutable.HashMap[String,Int] = Map(follow -> 5, establish -> 4, demand -> 1, sister -> 1, former -> 3, labour -> 2, dragons -> 1, divinity -> 1, dorset -> 1, uppsala -> 1, whitford -> 1, founding -> 2, danacord -> 1, witchcraft -> 1, trick -> 1, православ -> 1, famous -> 2, armstrong -> 1, ace -> 2, emigrate -> 1, expense -> 1, godowsky -> 1, cordial -> 1, sixth -> 1, hard -> 1, mincemeat -> 1, guam -> 1, lover -> 1, war -> 4, gap -> 1, mysterious -> 1, development -> 3, carry -> 4, obscenity -> 1, adler -> 1, medium -> 2, easier -> 2, betray -> 1, reagan -> 1, electoral -> 1, μαρτίου -> 1, sasebo -> 1, advance -> 2, therefore -> 1, редакцией -> 1, sorting -> 1, melee -> 1, soas -> 1, large -> 1, occipitus -> 1, salability -> 1, bishop -> 1, kanenobu -> 1, limit -..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zero = new HashMap[String, Int]()\n",
    "\n",
    "def merge(dfs: HashMap[String, Int], tfs: HashMap[String, Int]): HashMap[String, Int] = {\n",
    "    tfs.keySet.foreach(term => dfs += term -> (dfs.getOrElse(term, 0) + 1))\n",
    "    dfs\n",
    "}\n",
    "\n",
    "def comb(dfs1: HashMap[String, Int], dfs2: HashMap[String, Int]): HashMap[String, Int] = {\n",
    "    for((term, count) <- dfs2) dfs1 += term -> (dfs1.getOrElse(term, 0) + count)\n",
    "    dfs1\n",
    "}\n",
    "docTermFreqs.aggregate(zero)(merge, comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val docFreq = docTermFreqs.flatMap(_.keySet).map(_ -> 1).reduceByKey(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Computing IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val numDocs = docTermFreqs.flatMap(_.keySet).distinct.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math.log\n",
    "\n",
    "val numTerms = 50000\n",
    "implicit val ordering = Ordering.by[(String, Int), Int](_._2)\n",
    "val topDocFreqs = docFreq.top(numTerms)\n",
    "val idfs = topDocFreqs.map {\n",
    "    case (term, count) => (term, log(numDocs.toDouble / count))\n",
    "}.toMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing mllib Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scala.collection.JavaConversions._\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val bIdfs = sc.broadcast(idfs).value\n",
    "val idTerms = idfs.keys.zipWithIndex.toMap\n",
    "val termIds = idTerms.map(_.swap)\n",
    "val bTermIds = sc.broadcast(idTerms).value\n",
    "\n",
    "val termDocMatrix = docTermFreqs.map { termFreqs =>\n",
    "    val docTotalTerms = termFreqs.values().sum\n",
    "    val termScores = termFreqs.filter {\n",
    "        case (term, freq) => bTermIds.containsKey(term)\n",
    "    }.map {\n",
    "        case (term, freq) => bTermIds(term) -> (bIdfs(term) * termFreqs(term) / docTotalTerms)\n",
    "    }.toSeq\n",
    "    Vectors.sparse(bTermIds.size, termScores)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Caluculating Single Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70,4411)\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "val mat = new RowMatrix(termDocMatrix)\n",
    "println((mat.numRows, mat.numCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "termDocMatrix.cache()\n",
    "val mat = new RowMatrix(termDocMatrix)\n",
    "val k = 30 //TODO make it something real e.g.: 1000\n",
    "val svd = mat.computeSVD(k, computeU=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Important Concepts and Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Important Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scala.collection.mutable.ArrayBuffer[Seq[(String, Double)]] = ArrayBuffer(ArraySeq((ascaphus,0.5527121377320893), (blast,0.1530877351528438), (eotheod,0.051919568955162435), (borough,4.212207839471294E-12), (fairtrade,4.208688952900275E-12), (corsair,3.6377329100389932E-12), (aruj,3.6376557148443123E-12), (ferenc,3.11739392980237E-12), (munnich,3.1139864992146826E-12), (bethel,1.3773721746490608E-12), (acres,1.3759288847170481E-12), (springs,6.44888656364806E-13), (jemez,6.397139762359672E-13), (tag,2.2410545641449175E-13), (baggage,1.4397944642086102E-13), (code,1.2597648618717372E-13), (bag,1.1205272820724588E-13), (passenger,1.0672712713599708E-13), (bar,9.778690444189886E-14), (game,9.498651865058605E-14), (piano,9.489631302983526E-14), (vox,8.541778395709798E-14), (simon,8.0..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "val v = svd.V\n",
    "val topTerms = new ArrayBuffer[Seq[(String, Double)]]()\n",
    "val arr = v.toArray\n",
    "for (i <- 0 until k) {\n",
    "    val offs = i * v.numRows // numRows == n (number of words)\n",
    "    val termWeigts = arr.slice(offs, offs + v.numRows).zipWithIndex // (feature_weight -> feture_id)\n",
    "    val sorted = termWeigts.sortBy(- _._1) // sort by feature weight descending\n",
    "    topTerms += sorted.take(numTerms).map { // take top terms and resolve each term itself that represents one feature\n",
    "       case (score, id) => (termIds(id), score)\n",
    "    }\n",
    "}\n",
    "topTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132330\n",
      "132330\n",
      "(4411, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array[(Double, Int)] = Array((0.5527121377320893,233), (0.1530877351528438,3288), (0.051919568955162435,4326), (4.212207839471294E-12,4209), (4.208688952900275E-12,833), (3.6377329100389932E-12,1128), (3.6376557148443123E-12,165), (3.11739392980237E-12,2503), (3.1139864992146826E-12,2996), (1.3773721746490608E-12,178), (1.3759288847170481E-12,280), (6.44888656364806E-13,4251), (6.397139762359672E-13,312), (2.2410545641449175E-13,966), (1.4397944642086102E-13,3672), (1.2597648618717372E-13,2154), (1.1205272820724588E-13,660), (1.0672712713599708E-13,383), (9.778690444189886E-14,3041), (9.498651865058605E-14,3878), (9.489631302983526E-14,1548), (8.541778395709798E-14,2890), (8.074790835976842E-14,35), (6.949584137327935E-14,3135), (6.942189878511584E-14,352), (6.926230422532598E-14..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(svd.V.numRows * svd.V.numCols)\n",
    "println(svd.V.toArray.size)\n",
    "print(\"(\" + svd.V.numRows)\n",
    "println(\", \" + svd.V.numCols + \")\")\n",
    "svd.V.toArray.slice(0, v.numRows).zipWithIndex.sortBy(- _._1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Important Document - Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:76: error: value _1 is not a member of scala.collection.mutable.HashMap[String,Int]\n",
       "       val docIds = docTermFreqs.flatMap(_._1).zipWithUniqueId().map(_.swap).collectAsMap()\n",
       "                                           ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val u = svd.U\n",
    "val docIds = docTermFreqs.flatMap(_._1).zipWithUniqueId().map(_.swap).collectAsMap()\n",
    "val topDocs = new ArrayBuffer[Seq[(String, Double)]]()\n",
    "for (i <- 0 until k) {\n",
    "    val docWeights = u.rows.map(_.toArray(i)).zipWithUniqueId()\n",
    "    topDocs += docWeights.top(k).map {\n",
    "        case (score, id) => docIds(id) -> score\n",
    "    }\n",
    "}\n",
    "topDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scala.collection.mutable.Iterable[Int] = ArrayBuffer(1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val xs = new HashMap[Int, String]()\n",
    "xs += (1 -> \"hi\")\n",
    "xs.map(_._1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scala.collection.mutable.Iterable[String] = ArrayBuffer(philosophical, view)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docTermFreqs.take(1)(0).map(_._1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val topdocs = new ArrayBuffer[Seq[(String, Double)]]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying and Scoring with Low-Dimentional Respresentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-Term Relelvence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val normalizedVS = rowsNormalized(vsd.V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Document Relevence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-Document Relevence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutlible-Term Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10.4 (Spark 1.5.2)",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
