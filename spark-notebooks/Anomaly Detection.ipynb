{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rawData = sc.textFile(\"/docker/datasets/kddcup.data\").cache()//.sample(false, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((spark.localProperties.clone,true), (spark.cores.max,8 spark.executor.memory=12g), (spark.app.name,IBM Spark Kernel), (spark.driver.host,192.168.0.8), (spark.master,local[*]), (spark.executor.id,driver), (spark.submit.deployMode,client), (spark.repl.class.uri,http://192.168.0.8:38359), (spark.fileserver.uri,http://192.168.0.8:47359), (spark.externalBlockStore.folderName,spark-6cd3add5-80e1-4e47-99e2-3a1effebf71d), (spark.driver.port,56663), (spark.app.id,local-1450171299861), (spark.jars,file:/opt/spark-kernel/lib/kernel-assembly-0.1.5-SNAPSHOT.jar))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf.getAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0,tcp,http,SF,215,45076,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,0.00,0.00,1.00,0.00,0.00,0,0,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,normal."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData.map(_.split(',').last).countByValue.toSeq.sortBy(_._2).reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg._\n",
    "\n",
    "val labelsAndData = rawData.map { line =>\n",
    "    val buffer = line.split(',').toBuffer\n",
    "    buffer.remove(1, 3)\n",
    "    val label = buffer.remove(buffer.length-1)\n",
    "    val vector = Vectors.dense(buffer.map(_.toDouble).toArray)\n",
    "    (label, vector)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val data = labelsAndData.values.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train k-means model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.clustering._\n",
    "\n",
    "val kmeans = new KMeans()\n",
    "val model = kmeans.run(data)\n",
    "\n",
    "model.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val clusterLabel = labelsAndData.map { case (label, datum) =>\n",
    "    val cluster = model.predict(datum)\n",
    "    (cluster, label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val clusterLabelCounts = clusterLabel.countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusterLabelCounts.toSeq.sorted.foreach {\n",
    "    case ((cluster, label), count) => println(f\"$cluster%1s$label%18s$count\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating distances\n",
    "\n",
    "Calculating distances between 2 vectors using Euclidean distance function:\n",
    "$$ distance(A, B) = \\sqrt{\\sum (A-B)^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.mllib.clustering._\n",
    "\n",
    "def distance(a: Vector, b: Vector) = sqrt((a.toArray zip b.toArray).map { case (x, y) => pow(x-y, 2) }.sum)\n",
    "\n",
    "def distanceToCentroid(a: Vector, model: KMeansModel) = {\n",
    "    val cluster = model.predict(a)\n",
    "    val centroid = model.clusterCenters(cluster)\n",
    "    distance(a, centroid)\n",
    "}\n",
    "\n",
    "def clusterScore(data: RDD[Vector], k: Int) = {\n",
    "    val kmeans = new KMeans()\n",
    "    kmeans.setK(k)\n",
    "    val model = kmeans.run(data)\n",
    "    data.map(a => distanceToCentroid(a, model)).mean\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(5 to 40 by 5).map(k => k -> clusterScore(data, k)).toList.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Normalized Data\n",
    "Selecting a good value of *k* that returns a small *k* with a small distences avarage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature._\n",
    "\n",
    "val normalizer = new Normalizer(2)\n",
    "def normalize(a: Vector) = normalizer.transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val normalizedData = data.map(normalize).cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(60 to 120 by 10).par.map(k => k -> clusterScore(normalizedData, k)).toList.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring with Entropy\n",
    "_\n",
    "\n",
    "$$ entropy(v) = \\sum_i\\big(P(v_i)*\\log_2 P(v_i)\\big) $$\n",
    "$$ P(v_i) = \\dfrac{v_i}{\\sum{v}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math._\n",
    "\n",
    "def entropy(counts: Iterable[Int]) = {\n",
    "    val values = counts.filter(_ > 0)\n",
    "    val n: Double = values.sum\n",
    "    values.map { v =>\n",
    "        val p = v/n\n",
    "        -p * log(p)\n",
    "    }.sum\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clusterEntropyScore(dataAndLables: RDD[(String, Vector)], k: Int) = {\n",
    "    val kmeans = new KMeans()\n",
    "    kmeans.setK(k)\n",
    "    val model = kmeans.run(dataAndLables.values)\n",
    "    val labelsAndClusters = dataAndLables.mapValues(model.predict)\n",
    "    val clustersAndLabels = labelsAndClusters.map(_.swap)\n",
    "    val labelsInClusters = clustersAndLabels.groupByKey().values\n",
    "    val labelCountsInClusters = labelsInClusters.map(_.groupBy(l => l).map(_._2.size))\n",
    "    val n = dataAndLables.count\n",
    "    labelCountsInClusters.map(m => m.sum * entropy(m)).sum / n\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val normalizedDataAndLabels = labelsAndData.mapValues(normalize)\n",
    "(60 to 160 by 10).par.map(k => k -> clusterEntropyScore(normalizedDataAndLabels, k)).toList.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "env": {
   "SPARK_CONFIGURATION": "spark.cores.max=8 spark.executor.memory=12g"
  },
  "kernelspec": {
   "display_name": "Scala 2.10.4 (Spark 1.5.2)",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
